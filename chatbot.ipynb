{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple nanogpt chatbot example\n",
    "designed from minor changes over the sample.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run on colab, first install requirements\n",
    "!git clone https://github.com/karpathy/nanoGPT.git\n",
    "!pip install --quiet tiktoken transformers\n",
    "!pip install --quiet --pre torch --force-reinstall --index-url https://download.pytorch.org/whl/nightly/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "os.chdir('./nanoGPT')\n",
    "from model import GPTConfig, GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "the default model name is 'gpt2-medium' (350M params). You can change it to 'gpt2' (124M params), 'gpt2-large' (774M params) or 'gpt2-xl' (1558M params). The 'gpt2-xl' size exceeds RAM limit for free google colab accounts.\n",
    "\n",
    "max_new_tokens to 16. That makes the chat faster, but shorter replies.\n",
    "\n",
    "temperature decreased to 0.3. If higher values, sounds like an insane chatbot, you can try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_from = 'gpt2-medium'#'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "max_new_tokens = 16 # number of tokens generated in each sample\n",
    "temperature = 0.3 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 50 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "This part is not changed compared to the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the chatbot code\n",
    "\n",
    "enter 'exit' to leave the chat\n",
    "\n",
    "Note that the chatbot does not have any memory about the ongoing discussion. kinda redfish chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        while True:\n",
    "          prompt = input('Human > ')\n",
    "          if prompt == \"exit\":\n",
    "            break\n",
    "          x = (torch.tensor(encode(prompt), dtype=torch.long, device=device)[None, ...])\n",
    "          y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "          output = decode(y[0].tolist())\n",
    "          # post process the answer\n",
    "          # remove the prompt from the output\n",
    "          output = output.split(prompt)[1]\n",
    "          # split sentences, there might be repetitions, forget the end of the\n",
    "          # output, often looks like a pending sentence\n",
    "          # todo: add a repetition_penalty option in the generate method?\n",
    "          if '.' in output:\n",
    "            sentences = [s.strip() for s in output.split('.')[:-1]]\n",
    "            # remove duplicates\n",
    "            no_duplicates_sentences = []\n",
    "            for item in sentences:\n",
    "                if item not in no_duplicates_sentences:\n",
    "                    no_duplicates_sentences.append(item)\n",
    "            # then rebuild the paragraph as a set of unique sentences\n",
    "            output = '.'.join(no_duplicates_sentences)\n",
    "          # sometimes there's a character speaking, remove what is before ':'\n",
    "          if ':' in output:\n",
    "            output = output.split(':')[1]\n",
    "          # one more strip call, just to be sure about trailing whitespaces\n",
    "          output = output.strip()\n",
    "          print(\"Chatbot >\", output)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
